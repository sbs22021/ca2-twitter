{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c937df8-f78e-4480-9863-9dad62bfde43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://rmsryu-vm.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e700b1e-71d3-46e2-b762-a5de5d06dc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"MongoClient with metadata {\\\"driver\\\": {\\\"name\\\": \\\"mongo-java-driver|sync|mongo-spark-connector|source\\\", \\\"version\\\": \\\"4.9.1|10.1.1\\\"}, \\\"os\\\": {\\\"type\\\": \\\"Linux\\\", \\\"name\\\": \\\"Linux\\\", \\\"architecture\\\": \\\"amd64\\\", \\\"version\\\": \\\"5.15.0-1035-azure\\\"}, \\\"platform\\\": \\\"Java/Ubuntu/11.0.18+10-post-Ubuntu-0ubuntu120.04.1|Scala/2.12.15/Spark/3.2.3\\\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6df0fc06, com.mongodb.Jep395RecordCodecProvider@6c385b09]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[hadoop-vm.internal.cloudapp.net:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\",\"thread_name\":\"Thread-4\",\"@timestamp\":\"2023-05-03T18:02:25.753+0000\",\"level\":\"INFO\",\"logger_name\":\"org.mongodb.driver.client\"}\n",
      "{\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Cluster description not yet available. Waiting for 30000 ms before timing out\",\"thread_name\":\"Thread-4\",\"@timestamp\":\"2023-05-03T18:02:25.754+0000\",\"level\":\"INFO\",\"logger_name\":\"org.mongodb.driver.cluster\"}\n",
      "{\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Monitor thread successfully connected to server with description ServerDescription{address=hadoop-vm.internal.cloudapp.net:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=17, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2390768}\",\"thread_name\":\"cluster-ClusterId{value='6452a1b12867853324307a2a', description='null'}-hadoop-vm.internal.cloudapp.net:27017\",\"@timestamp\":\"2023-05-03T18:02:25.761+0000\",\"level\":\"INFO\",\"logger_name\":\"org.mongodb.driver.cluster\"}\n",
      "{\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Creating MongoTable: mongo-spark-connector-10.1.1\",\"thread_name\":\"Thread-4\",\"@timestamp\":\"2023-05-03T18:02:55.535+0000\",\"level\":\"INFO\",\"logger_name\":\"com.mongodb.spark.sql.connector.MongoTable\"}\n",
      "root\n",
      " |-- _id: long (nullable = true)\n",
      " |-- coordinates: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- geo: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp_ms: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext, functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "mongo_uri = \"mongodb://hadoop-vm.internal.cloudapp.net:27017/ca2.tweets\"\n",
    "\n",
    "# Spark version 3.2.3\n",
    "# MongoDB version 6.0.5\n",
    "# Java Version 11\n",
    "\n",
    "# create a spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Tweets') \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", mongo_uri) \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", mongo_uri) \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.1,org.mongodb:mongodb-driver-core:4.9.1,org.mongodb:bson:4.9.1,org.mongodb:mongodb-driver-core:4.9.1,org.mongodb:mongodb-driver-sync:4.9.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# read data from mongodb collection \"tweets\" into a dataframe \"df\"\n",
    "df = spark.read \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .option(\"connection.uri\", mongo_uri) \\\n",
    "    .option(\"database\", \"ca2\") \\\n",
    "    .option(\"collection\", \"tweets\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f238379-a72e-4a05-8559-1cca61466e9e",
   "metadata": {},
   "source": [
    "# Create a timeseries collection in Mongo DB\n",
    "\n",
    "\n",
    "## Step 1: Mongo shell create timeseries_tweets collection\n",
    "```javascript\n",
    "use ca2;\n",
    "\n",
    "db.createCollection(\"timeseries_tweets\", {\n",
    "  \"timeseries\": {\n",
    "    \"timeField\": \"timestamp\",\n",
    "    \"metaField\": null, // Set to null if you don't have any metadata fields\n",
    "    \"granularity\": \"seconds\" // Choose the appropriate granularity, e.g., seconds, minutes, hours\n",
    "  }\n",
    "});\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649888ea-93d8-46f8-b81b-5f943c97ebe6",
   "metadata": {},
   "source": [
    "## Step 2: On existing tweets collection create a new timestamp field \n",
    "\n",
    "```javascript\n",
    "db.tweets.find().forEach(function (doc) {\n",
    "  var timestamp = new Date(Number(doc.timestamp_ms));\n",
    "  var updateDoc = { $set: { \"timestamp\": timestamp } };\n",
    "  db.tweets.updateOne({ \"_id\": doc._id }, updateDoc);\n",
    "});\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc9833-990d-4c85-891d-c4c79427eee4",
   "metadata": {},
   "source": [
    "## Step 3: Migrate tweets collection to timeseries_tweets\n",
    "\n",
    "```javascript\n",
    "db.tweets.find().forEach(function (doc) {\n",
    "  db.timeseries_tweets.insertOne(doc);\n",
    "});\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b5590-9eec-4067-9768-3bed3ef930b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load timeseries_tweets collection\n",
    "df = spark.read \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .option(\"connection.uri\", mongo_uri) \\\n",
    "    .option(\"database\", \"ca2\") \\\n",
    "    .option(\"collection\", \"timeseries_tweets\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580a75b-1554-42a3-b1c6-9a3b5398b7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
