{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b5c7246-de85-4727-bfc1-795974ed0a49",
   "metadata": {},
   "source": [
    "# Create a catalogue of tweets in mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd190455-cc90-495e-9c30-04137c2eb5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://rmsryu-vm.internal.cloudapp.net:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e47c8d7d-d0c8-46a0-a881-bfcf6ae10421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f91a7c-1d82-4980-81b5-5a7cd9f6ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from hdfs import InsecureClient\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# MONGODB\n",
    "mongo_url = 'mongodb://hadoop-vm.internal.cloudapp.net:27017/ca2.tweets'\n",
    "database_name = 'ca2'\n",
    "collection_name = 'tweets'\n",
    "\n",
    "# HADOOP\n",
    "hdfs_url = 'http://hadoop-vm.internal.cloudapp.net:9870'\n",
    "hdfs_directory = \"/twitter\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3b781-ff30-4147-885e-edaaad538de4",
   "metadata": {},
   "source": [
    "## Connect and create database and tweets collection\n",
    "Include index on field id for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ec7d4-219f-4403-aa53-0b47f28b785e",
   "metadata": {},
   "source": [
    "# Read tweets from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e75b06-06ef-4a6e-9b8d-b4482dc15133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597084667896\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597084667896\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34358\",\"thread_name\":\"shuffle-server-8-2\",\"@timestamp\":\"2023-05-03T09:01:49.821+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597084667896\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597084667896\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34374\",\"thread_name\":\"shuffle-server-8-3\",\"@timestamp\":\"2023-05-03T09:01:49.830+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34388\",\"thread_name\":\"shuffle-server-8-4\",\"@timestamp\":\"2023-05-03T09:01:49.836+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112598577840121\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112598577840121\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34394\",\"thread_name\":\"shuffle-server-8-1\",\"@timestamp\":\"2023-05-03T09:01:49.838+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112596195475448\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112596195475448\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34410\",\"thread_name\":\"shuffle-server-8-2\",\"@timestamp\":\"2023-05-03T09:01:49.840+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34414\",\"thread_name\":\"shuffle-server-8-3\",\"@timestamp\":\"2023-05-03T09:01:49.842+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112596195475448\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112596195475448\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34428\",\"thread_name\":\"shuffle-server-8-4\",\"@timestamp\":\"2023-05-03T09:01:49.844+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34436\",\"thread_name\":\"shuffle-server-8-1\",\"@timestamp\":\"2023-05-03T09:01:49.851+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586111495542996984\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586111495542996984\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34448\",\"thread_name\":\"shuffle-server-8-2\",\"@timestamp\":\"2023-05-03T09:01:49.853+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586111497220718584\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586111497220718584\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34450\",\"thread_name\":\"shuffle-server-8-3\",\"@timestamp\":\"2023-05-03T09:01:49.855+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112598007414777\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112598007414777\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34454\",\"thread_name\":\"shuffle-server-8-4\",\"@timestamp\":\"2023-05-03T09:01:49.857+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112599852908537\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112599852908537\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:34470\",\"thread_name\":\"shuffle-server-8-1\",\"@timestamp\":\"2023-05-03T09:01:49.860+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597084667896\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597084667896\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57836\",\"thread_name\":\"rpc-server-4-3\",\"@timestamp\":\"2023-05-03T09:02:08.009+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597084667896\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597084667896\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57852\",\"thread_name\":\"rpc-server-4-4\",\"@timestamp\":\"2023-05-03T09:02:08.016+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57864\",\"thread_name\":\"rpc-server-4-1\",\"@timestamp\":\"2023-05-03T09:02:08.019+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112598577840121\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112598577840121\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57874\",\"thread_name\":\"rpc-server-4-2\",\"@timestamp\":\"2023-05-03T09:02:08.022+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112596195475448\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112596195475448\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57886\",\"thread_name\":\"rpc-server-4-3\",\"@timestamp\":\"2023-05-03T09:02:08.024+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57888\",\"thread_name\":\"rpc-server-4-4\",\"@timestamp\":\"2023-05-03T09:02:08.027+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112596195475448\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112596195475448\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57896\",\"thread_name\":\"rpc-server-4-1\",\"@timestamp\":\"2023-05-03T09:02:08.028+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57904\",\"thread_name\":\"rpc-server-4-2\",\"@timestamp\":\"2023-05-03T09:02:08.030+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586111495542996984\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586111495542996984\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57916\",\"thread_name\":\"rpc-server-4-3\",\"@timestamp\":\"2023-05-03T09:02:08.033+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586111497220718584\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586111497220718584\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57930\",\"thread_name\":\"rpc-server-4-4\",\"@timestamp\":\"2023-05-03T09:02:08.036+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112598007414777\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112598007414777\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57934\",\"thread_name\":\"rpc-server-4-1\",\"@timestamp\":\"2023-05-03T09:02:08.038+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112599852908537\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112599852908537\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:57940\",\"thread_name\":\"rpc-server-4-2\",\"@timestamp\":\"2023-05-03T09:02:08.041+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 43888)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 256, in authenticate_and_accum_updates\n",
      "    raise ValueError(\n",
      "ValueError: The value of the provided token to the AccumulatorServer is not correct.\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 43904)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 4: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 43914)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 4: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 43926)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8c in position 13: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 43932)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe1 in position 11: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 43934)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xaf in position 4: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 43946)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position 11: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 43962)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xaf in position 4: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 43976)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xad in position 12: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 43990)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xcc in position 12: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 44000)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position 4: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 44010)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd3 in position 11: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 44016)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x89 in position 4: invalid start byte\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextended_tweet\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tweet:\n\u001b[1;32m     27\u001b[0m     tweet_p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tweet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextended_tweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$set\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtweet_p\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/collection.py:1041\u001b[0m, in \u001b[0;36mCollection.update_one\u001b[0;34m(self, filter, update, upsert, bypass_document_validation, collation, array_filters, hint, session, let, comment)\u001b[0m\n\u001b[1;32m   1037\u001b[0m common\u001b[38;5;241m.\u001b[39mvalidate_list_or_none(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray_filters\u001b[39m\u001b[38;5;124m\"\u001b[39m, array_filters)\n\u001b[1;32m   1039\u001b[0m write_concern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_concern_for(session)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m UpdateResult(\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_retryable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupsert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbypass_doc_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbypass_document_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray_filters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marray_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1054\u001b[0m     write_concern\u001b[38;5;241m.\u001b[39macknowledged,\n\u001b[1;32m   1055\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/collection.py:836\u001b[0m, in \u001b[0;36mCollection._update_retryable\u001b[0;34m(self, criteria, document, upsert, multi, write_concern, op_id, ordered, bypass_doc_val, collation, array_filters, hint, session, let, comment)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(session, sock_info, retryable_write):\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update(\n\u001b[1;32m    818\u001b[0m         sock_info,\n\u001b[1;32m    819\u001b[0m         criteria,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m         comment\u001b[38;5;241m=\u001b[39mcomment,\n\u001b[1;32m    834\u001b[0m     )\n\u001b[0;32m--> 836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retryable_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macknowledged\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmulti\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/mongo_client.py:1476\u001b[0m, in \u001b[0;36mMongoClient._retryable_write\u001b[0;34m(self, retryable, func, session)\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;124;03m\"\"\"Internal retryable write helper.\"\"\"\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tmp_session(session) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[0;32m-> 1476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_with_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/mongo_client.py:1349\u001b[0m, in \u001b[0;36mMongoClient._retry_with_session\u001b[0;34m(self, retryable, func, session, bulk)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m\"\"\"Execute an operation with at most one consecutive retries\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \n\u001b[1;32m   1341\u001b[0m \u001b[38;5;124;03mReturns func()'s return value on success. On error retries the same\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;124;03mRe-raises any exception thrown by func().\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m retryable \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1347\u001b[0m     retryable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mretry_writes \u001b[38;5;129;01mand\u001b[39;00m session \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m session\u001b[38;5;241m.\u001b[39min_transaction\n\u001b[1;32m   1348\u001b[0m )\n\u001b[0;32m-> 1349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbulk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/_csot.py:105\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/mongo_client.py:1390\u001b[0m, in \u001b[0;36mMongoClient._retry_internal\u001b[0;34m(self, retryable, func, session, bulk)\u001b[0m\n\u001b[1;32m   1388\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m last_error\n\u001b[1;32m   1389\u001b[0m             retryable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1390\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretryable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[1;32m   1392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_retrying():\n\u001b[1;32m   1393\u001b[0m         \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[1;32m   1394\u001b[0m         \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m         \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/collection.py:817\u001b[0m, in \u001b[0;36mCollection._update_retryable.<locals>._update\u001b[0;34m(session, sock_info, retryable_write)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(session, sock_info, retryable_write):\n\u001b[0;32m--> 817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupsert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mordered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mordered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbypass_doc_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbypass_doc_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray_filters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marray_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretryable_write\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/collection.py:773\u001b[0m, in \u001b[0;36mCollection._update\u001b[0;34m(self, sock_info, criteria, document, upsert, multi, write_concern, op_id, ordered, bypass_doc_val, collation, array_filters, hint, session, retryable_write, let, comment)\u001b[0m\n\u001b[1;32m    769\u001b[0m     command[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbypassDocumentValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# The command result has to be published for APM unmodified\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;66;03m# so we make a shallow copy here before adding updatedExisting.\u001b[39;00m\n\u001b[0;32m--> 773\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msock_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__write_response_codec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretryable_write\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    782\u001b[0m _check_write_command_response(result)\n\u001b[1;32m    783\u001b[0m \u001b[38;5;66;03m# Add the updatedExisting field for compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/pool.py:795\u001b[0m, in \u001b[0;36mSocketInfo.command\u001b[0;34m(self, dbname, spec, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Catch socket.error, KeyboardInterrupt, etc. and close ourselves.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m--> 795\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_connection_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/pool.py:767\u001b[0m, in \u001b[0;36mSocketInfo.command\u001b[0;34m(self, dbname, spec, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_not_writable(unacknowledged)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdbname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_mongos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallowable_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlisteners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bson_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_op_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop_msg_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43munacknowledged\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munacknowledged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexhaust_allowed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexhaust_allowed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (OperationFailure, NotPrimaryError):\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/network.py:156\u001b[0m, in \u001b[0;36mcommand\u001b[0;34m(sock_info, dbname, spec, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields, exhaust_allowed, write_concern)\u001b[0m\n\u001b[1;32m    154\u001b[0m     response_doc \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[43mreceive_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     sock_info\u001b[38;5;241m.\u001b[39mmore_to_come \u001b[38;5;241m=\u001b[39m reply\u001b[38;5;241m.\u001b[39mmore_to_come\n\u001b[1;32m    158\u001b[0m     unpacked_docs \u001b[38;5;241m=\u001b[39m reply\u001b[38;5;241m.\u001b[39munpack_response(\n\u001b[1;32m    159\u001b[0m         codec_options\u001b[38;5;241m=\u001b[39mcodec_options, user_fields\u001b[38;5;241m=\u001b[39muser_fields\n\u001b[1;32m    160\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/network.py:217\u001b[0m, in \u001b[0;36mreceive_message\u001b[0;34m(sock_info, request_id, max_message_size)\u001b[0m\n\u001b[1;32m    214\u001b[0m         deadline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Ignore the response's request id.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m length, _, response_to, op_code \u001b[38;5;241m=\u001b[39m _UNPACK_HEADER(\n\u001b[0;32m--> 217\u001b[0m     \u001b[43m_receive_data_on_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# No request_id for exhaust cursor \"getMore\".\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/py38_default/lib/python3.8/site-packages/pymongo/network.py:299\u001b[0m, in \u001b[0;36m_receive_data_on_socket\u001b[0;34m(sock_info, length, deadline)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _csot\u001b[38;5;241m.\u001b[39mget_timeout():\n\u001b[1;32m    298\u001b[0m         sock_info\u001b[38;5;241m.\u001b[39mset_socket_timeout(\u001b[38;5;28mmax\u001b[39m(deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic(), \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 299\u001b[0m     chunk_length \u001b[38;5;241m=\u001b[39m \u001b[43msock_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbytes_read\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BLOCKING_IO_ERRORS:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimed out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Connect to Hadoop\n",
    "client = InsecureClient(hdfs_url, user='hduser')\n",
    "\n",
    "# Connect to MongoDB\n",
    "mongo_client = MongoClient(mongo_url)\n",
    "db = mongo_client[database_name]\n",
    "collection = db[collection_name]\n",
    "\n",
    "# List JSON files in Hadoop directory\n",
    "json_files = client.list(hdfs_directory)\n",
    "\n",
    "# Read JSON files and insert into MongoDB\n",
    "for file_name in json_files:\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = f\"{hdfs_directory}/{file_name}\"\n",
    "        print(f\"processing {file_path}\")\n",
    "        with client.read(file_path) as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    tweet = json.loads(line)\n",
    "                    tweet_p = {\n",
    "                        '_id': tweet['id'],\n",
    "                        'text': tweet.get('text'),\n",
    "                        'timestamp_ms': tweet.get('timestamp_ms'),\n",
    "                        'geo': tweet.get('geo'),\n",
    "                        'coordinates': tweet.get('coordinates'),\n",
    "                    }\n",
    "                    if 'extended_tweet' in tweet:\n",
    "                        tweet_p['full_text'] = tweet['extended_tweet'].get('full_text')\n",
    "\n",
    "                    collection.update_one({'_id': tweet['id']}, {'$set': tweet_p}, upsert=True)\n",
    "                except:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37e5fa-0157-426c-b919-7367b7bc8c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
