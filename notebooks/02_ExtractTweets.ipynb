{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b6e50c-9256-45bb-a51d-83a8d705b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "hdfs_directory = 'hdfs://hadoop-vm.internal.cloudapp.net:9000/twitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1181246-53bd-49ed-b49a-36e9c687815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95747c56-684c-4f32-9951-8392cf19bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import bz2\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from hdfs import InsecureClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c6d2742-d7c8-4852-b347-74c61e43a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now() # current date and time\n",
    "date_time = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "logging.basicConfig(filename=f'extract-{date_time}.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18de0ede-48ff-40ad-81d8-74f5482339a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.error(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1503b89a-e29c-4417-a1bf-3cb5b675645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['COVID-19', 'Coronavirus', 'Pandemic', 'Vaccine', 'Vaccination', 'Immunization', 'COVID vaccine', 'Vaccine rollout', 'Vaccine hesitancy', 'Vaccine mandate', 'Booster shot', 'Vaccine passport', 'Vaccination rate', 'Public health']\n",
    "keywords_laboratories = ['Moderna', 'Pfizer', 'AstraZeneca', 'Johnson & Johnson', 'Sinovac', 'Sinopharm', 'Novavax', 'Sanofi', 'GlaxoSmithKline', 'BioNTech']\n",
    "unique_keywords = list(set(keywords + keywords_laboratories))\n",
    "\n",
    "hashtags = ['#COVID19', '#Coronavirus', '#Pandemic', '#Vaccine', '#Vaccination', '#GetVaccinated', '#COVIDVaccine', '#Immunization', '#VaccineHesitancy', '#VaccineMandate', '#BoosterShot', '#VaccinePassport', '#PublicHealth', '#StaySafe', '#VaccineRollout', '#VaccineTrials', '#ClinicalTrials', '#VaccineDevelopment', '#VaccineResearch', '#VaccineProduction', '#VaccineDistribution', '#VaccineSupplyChain', '#VaccineManufacturing', '#VaccineStorage', '#VaccineEfficacy', '#VaccineSafety', '#AntibodyTests', '#DiagnosticTests', '#PCRTests', '#SerologyTests', '#Immunology', '#HerdImmunity', '#VaccineNationalism']\n",
    "\n",
    "hashtags_laboratories = ['#Moderna', '#Pfizer', '#BioNTech', '#AstraZeneca', '#JohnsonAndJohnson', '#Janssen', '#Novavax', '#Sinovac', '#Sinopharm', '#Sanofi', '#GSK']\n",
    "unique_hashtags = list(set(hashtags + hashtags_laboratories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20dc3ca1-e898-40cf-b1cf-dbe81ee7f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_file(file_path, destination_folder):\n",
    "    with zipfile.ZipFile(file_path,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(destination_folder)\n",
    "        \n",
    "def extract_tar_file(file_path, destination_folder):\n",
    "    with tarfile.open(file_path, 'r') as tar:\n",
    "        tar.extractall(path=destination_folder)\n",
    "        \n",
    "def extract_bz2_file(file_path, destination_folder):\n",
    "    extracted_file_path = \"\"\n",
    "    with bz2.open(file_path, 'rt') as f_in:\n",
    "        file_name = os.path.basename(file_path).replace('.bz2', '')\n",
    "        extracted_file_path = os.path.join(destination_folder, file_name)\n",
    "        \n",
    "        with open(extracted_file_path, 'w') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "            \n",
    "    return extracted_file_path\n",
    "\n",
    "def get_all_files(directory, recursive = False):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if(recursive):\n",
    "            for d in dirs:\n",
    "                file_list = file_list + get_all_files(d,recursive)\n",
    "        \n",
    "        #files\n",
    "        for file in files:\n",
    "            file_list.append(os.path.join(root, file))\n",
    "    return file_list\n",
    "\n",
    "def create_folder(folder):\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d5267a3-4bff-4bd6-96ad-25dd625a1381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_text\n"
     ]
    }
   ],
   "source": [
    "tweet_str = '{\"text\": \"text\", \"extended_tweet\": {\"full_text\":\"full_text\"}}'\n",
    "tweet = json.loads(tweet_str)\n",
    "tweet['extended_tweet']['full_text']\n",
    "extended_tweet = tweet.get('extended_tweet')\n",
    "if extended_tweet:\n",
    "    print(extended_tweet.get('full_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9e3c94a-eb0e-4cee-82d8-d1f168ae67d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'text', 'extended_tweet': {'full_text': 'full_text'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "520cee85-be81-4121-9139-f52970d05fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"data-01/output\"\n",
    "tweets_dir = \"data-01/tweets\"\n",
    "\n",
    "shutil.rmtree(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611eb373-fd6b-4d80-a9dc-0fa77360f454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/twitter_stream_2020_01_01.tar\n",
      "Processing extracted files\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"data-01/output\"\n",
    "tweets_dir = \"data-01/tweets\"\n",
    "\n",
    "create_folder(tweets_dir)\n",
    "create_folder(output_dir)\n",
    "\n",
    "idx = 0\n",
    "for data_folder in [\"data\",\"data-01\"]:\n",
    "    for file in get_all_files(data_folder):\n",
    "        if(\"twitter\" in file):\n",
    "            create_folder(output_dir)\n",
    "            \n",
    "            # Extract file\n",
    "            print(f\"Extracting {file}\")\n",
    "            if(\"zip\" in file):\n",
    "                extract_zip_file(file, output_dir)\n",
    "            else:\n",
    "                extract_tar_file(file, output_dir)\n",
    "            \n",
    "            \n",
    "            print(f\"Processing extracted files\")\n",
    "            idx+=1\n",
    "            output_file = f\"{tweets_dir}/covid-tweets-{idx}.json\"\n",
    "            # Open final file to save tweets on zip\n",
    "            with open(output_file, 'x') as f_out:               \n",
    "                for file in get_all_files(output_dir, recursive=True):\n",
    "                    if file.endswith('.bz2'):\n",
    "                        extracted_file_path = extract_bz2_file(file, tweets_dir)\n",
    "                        # Read the file\n",
    "                        with open(extracted_file_path, 'r') as f_in:\n",
    "                            for line in f_in:\n",
    "                                tweet = json.loads(line)\n",
    "                                # Check if tweet contains a keyword or hashtag\n",
    "                                try:\n",
    "                                    # Get full tweet text when posible\n",
    "                                    t_text = tweet['text']\n",
    "                                    try:\n",
    "                                        t_text = tweet['extended_tweet']['full_text']\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                    \n",
    "                                    if any(keyword in t_text for keyword in keywords) or any(hashtag in t_text for hashtag in hashtags):\n",
    "                                        json.dump(tweet, f_out)\n",
    "                                        f_out.write('\\n')\n",
    "                                except:\n",
    "                                    pass\n",
    "                        # remove extracted file\n",
    "                        os.remove(extracted_file_path)\n",
    "            # clean up output\n",
    "            shutil.rmtree(output_dir)\n",
    "            break         \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
