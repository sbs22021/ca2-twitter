{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c937df8-f78e-4480-9863-9dad62bfde43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.warehouse.dir',\n",
       "  'file:/home/rmsryu/notebooks/CA2/ca2-twitter/notebooks/spark-warehouse'),\n",
       " ('spark.jars',\n",
       "  'file:///opt/azure-storage-jars/azure-storage-2.0.0.jar,file:///opt/azure-storage-jars/hadoop-azure-2.7.5.jar,file:///dsvm/tools/spark/spark-3.2.3/jars/mongo-spark-connector_2.13-10.1.1.jar'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://rmsryu-vm.internal.cloudapp.net:45523/jars/azure-storage-2.0.0.jar,spark://rmsryu-vm.internal.cloudapp.net:45523/jars/mongo-spark-connector_2.13-10.1.1.jar,spark://rmsryu-vm.internal.cloudapp.net:45523/jars/hadoop-azure-2.7.5.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '5g'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.driver.port', '45523'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///opt/azure-storage-jars/azure-storage-2.0.0.jar,file:///opt/azure-storage-jars/hadoop-azure-2.7.5.jar,file:///dsvm/tools/spark/spark-3.2.3/jars/mongo-spark-connector_2.13-10.1.1.jar'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.startTime', '1683103708072'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.id', 'local-1683103709117'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'rmsryu-vm.internal.cloudapp.net')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e700b1e-71d3-46e2-b762-a5de5d06dc3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o138.load.\n: java.lang.NoClassDefFoundError: org/bson/BsonValue\n\tat com.mongodb.spark.sql.connector.MongoTableProvider.inferSchema(MongoTableProvider.java:62)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:81)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:233)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: org.bson.BsonValue\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m      9\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTweets\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.mongodb.write.connection.uri\u001b[39m\u001b[38;5;124m\"\u001b[39m, mongo_uri) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# read data from mongodb collection \"questions\" into a dataframe \"df\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmongodb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmongo_uri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatabase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mca2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtweets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m/dsvm/tools/spark/current/python/pyspark/sql/readwriter.py:164\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/dsvm/tools/spark/current/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/dsvm/tools/spark/current/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/dsvm/tools/spark/current/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o138.load.\n: java.lang.NoClassDefFoundError: org/bson/BsonValue\n\tat com.mongodb.spark.sql.connector.MongoTableProvider.inferSchema(MongoTableProvider.java:62)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:81)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:233)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: org.bson.BsonValue\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\t... 18 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597084667896\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597084667896\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47242\",\"thread_name\":\"shuffle-server-8-2\",\"@timestamp\":\"2023-05-03T09:02:31.619+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597084667896\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597084667896\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47256\",\"thread_name\":\"shuffle-server-8-3\",\"@timestamp\":\"2023-05-03T09:02:31.630+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47260\",\"thread_name\":\"shuffle-server-8-4\",\"@timestamp\":\"2023-05-03T09:02:31.634+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112598577840121\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112598577840121\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47276\",\"thread_name\":\"shuffle-server-8-1\",\"@timestamp\":\"2023-05-03T09:02:31.637+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112596195475448\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112596195475448\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47292\",\"thread_name\":\"shuffle-server-8-2\",\"@timestamp\":\"2023-05-03T09:02:31.639+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47302\",\"thread_name\":\"shuffle-server-8-3\",\"@timestamp\":\"2023-05-03T09:02:31.641+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112596195475448\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112596195475448\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47314\",\"thread_name\":\"shuffle-server-8-4\",\"@timestamp\":\"2023-05-03T09:02:31.643+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47322\",\"thread_name\":\"shuffle-server-8-1\",\"@timestamp\":\"2023-05-03T09:02:31.645+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586111495542996984\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586111495542996984\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47330\",\"thread_name\":\"shuffle-server-8-2\",\"@timestamp\":\"2023-05-03T09:02:31.647+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586111497220718584\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586111497220718584\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47346\",\"thread_name\":\"shuffle-server-8-3\",\"@timestamp\":\"2023-05-03T09:02:31.649+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112598007414777\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112598007414777\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47358\",\"thread_name\":\"shuffle-server-8-4\",\"@timestamp\":\"2023-05-03T09:02:31.651+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112599852908537\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112599852908537\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:47370\",\"thread_name\":\"shuffle-server-8-1\",\"@timestamp\":\"2023-05-03T09:02:31.653+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34838)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 256, in authenticate_and_accum_updates\n",
      "    raise ValueError(\n",
      "ValueError: The value of the provided token to the AccumulatorServer is not correct.\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34844)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 4: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34850)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 4: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34864)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x9f in position 11: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34878)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 12-13: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34890)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xaf in position 4: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34898)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 13: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34900)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xaf in position 4: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34916)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xae in position 11: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34926)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xda in position 12: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34934)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position 4: invalid continuation byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34942)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc1 in position 16: invalid start byte\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 34946)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/anaconda/envs/py38_default/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 260, in handle\n",
      "    poll(authenticate_and_accum_updates)\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/dsvm/tools/spark/current/python/pyspark/accumulators.py\", line 250, in authenticate_and_accum_updates\n",
      "    received_token = received_token.decode(\"utf-8\")\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x89 in position 4: invalid start byte\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597084667896\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597084667896\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55730\",\"thread_name\":\"rpc-server-4-3\",\"@timestamp\":\"2023-05-03T09:03:16.118+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597084667896\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597084667896\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55734\",\"thread_name\":\"rpc-server-4-4\",\"@timestamp\":\"2023-05-03T09:03:16.125+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55740\",\"thread_name\":\"rpc-server-4-1\",\"@timestamp\":\"2023-05-03T09:03:16.128+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112598577840121\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112598577840121\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55746\",\"thread_name\":\"rpc-server-4-2\",\"@timestamp\":\"2023-05-03T09:03:16.130+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112596195475448\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112596195475448\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55750\",\"thread_name\":\"rpc-server-4-3\",\"@timestamp\":\"2023-05-03T09:03:16.132+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55760\",\"thread_name\":\"rpc-server-4-4\",\"@timestamp\":\"2023-05-03T09:03:16.134+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112596195475448\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112596195475448\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55774\",\"thread_name\":\"rpc-server-4-1\",\"@timestamp\":\"2023-05-03T09:03:16.139+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112597873197049\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112597873197049\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55782\",\"thread_name\":\"rpc-server-4-2\",\"@timestamp\":\"2023-05-03T09:03:16.142+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586111495542996984\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586111495542996984\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55784\",\"thread_name\":\"rpc-server-4-3\",\"@timestamp\":\"2023-05-03T09:03:16.145+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586111497220718584\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586111497220718584\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55800\",\"thread_name\":\"rpc-server-4-4\",\"@timestamp\":\"2023-05-03T09:03:16.204+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112598007414777\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112598007414777\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55812\",\"thread_name\":\"rpc-server-4-1\",\"@timestamp\":\"2023-05-03T09:03:16.206+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n",
      "{\"exception\":{\"exception_class\":\"java.lang.IllegalArgumentException\",\"exception_message\":\"Too large frame: 1586112599852908537\",\"stacktrace\":\"java.lang.IllegalArgumentException: Too large frame: 1586112599852908537\\n\\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\\n\\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\\n\\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\\n\\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\\n\\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\\n\\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\\n\\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\\n\\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\\n\\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\\n\\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\\n\\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\"},\"@version\":1,\"source_host\":\"rmsryu-vm\",\"message\":\"Exception in connection from /10.0.0.4:55814\",\"thread_name\":\"rpc-server-4-2\",\"@timestamp\":\"2023-05-03T09:03:16.209+0000\",\"level\":\"WARN\",\"logger_name\":\"org.apache.spark.network.server.TransportChannelHandler\"}\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext, functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "mongo_uri = \"mongodb://hadoop-vm.internal.cloudapp.net:27017/ca2\"\n",
    "\n",
    "# create a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Tweets\") \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", mongo_uri) \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", mongo_uri) \\\n",
    "    #.config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector:10.1.1') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# read data from mongodb collection \"questions\" into a dataframe \"df\"\n",
    "df = spark.read \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .option(\"uri\", mongo_uri) \\\n",
    "    .option(\"database\", \"ca2\") \\\n",
    "    .option(\"collection\", \"tweets\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206087be-b77c-455e-b2a5-ab8318a5ee4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
