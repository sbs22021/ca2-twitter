{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3d2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a40581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import tweetnlp\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pymongo import MongoClient\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f22a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = \"mongodb://hadoop-vm.internal.cloudapp.net:27017/ca2\"\n",
    "\n",
    "# Spark version 3.2.3\n",
    "# MongoDB version 6.0.5\n",
    "# Java Version 11\n",
    "\n",
    "# create a spark session\n",
    "# Jars dependencies available in maven repository\n",
    "# https://mvnrepository.com/search?q=mongodb-driver-sync\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Tweets') \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", mongo_uri) \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", mongo_uri) \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.1\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb:mongodb-driver-core:4.9.1\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb:mongodb-driver-sync:4.9.1\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb:bson:4.9.1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb5f09a-c602-4444-bf69-18beec71fece",
   "metadata": {},
   "source": [
    "# Load vaccine tweets from MongoDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36a8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from mongodb collection \"tweets\" into a dataframe \"df\"\n",
    "df = spark.read \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .option(\"connection.uri\", mongo_uri) \\\n",
    "    .option(\"database\", \"ca2\") \\\n",
    "    .option(\"collection\", \"vaccin_tweets_2_202004\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c010bf1",
   "metadata": {},
   "source": [
    "# Sentiment analysis with pretrained model \n",
    "\n",
    "https://aclanthology.org/2020.findings-emnlp.148\n",
    "\n",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7f5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tweetnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e22fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = tweetnlp.Sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379567c4-2152-4740-9d77-19b067c16972",
   "metadata": {},
   "source": [
    "## Data preparation for Sentiment Analysis\n",
    "\n",
    "- Remove user info.\n",
    "- Remove links\n",
    "- Leave all other characters unchanged including emojis as Robert is trained to interpret them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1824cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess tweets to remove user info and any web url as Roberta \n",
    "def parse_tweet(tweet):\n",
    "    new_text = []\n",
    "    for t in tweet.split(\" \"):\n",
    "        t = \"@user\" if t.startswith(\"@\") and len(t) > 1 else t\n",
    "        t = \"http\" if t.startswith(\"http\") else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# generate sentiment scores\n",
    "def sentiment_analyzer(text, model):\n",
    "    output = model.sentiment(text, return_probability=True)\n",
    "    return (output['label'], output['probability'][output['label']])\n",
    "\n",
    "def process_data(df, start_index, end_index, skip_to=0):\n",
    "    total_count = df_pd['_id'].count()\n",
    "    num_batches = (total_count // batch_size) + 1\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        if(start < skip_to):\n",
    "            continue\n",
    "        \n",
    "        print(f\"processing from {start} to {end}...\")\n",
    "        # Select the batch of data\n",
    "        batch_df = df[(df['index'] >= start) & (df['index'] < end)]\n",
    "        \n",
    "        # Apply the sentiment function to 'text' column\n",
    "        batch_df['sentiment'], batch_df['s_probability'] = zip(*batch_df['text'].progress_apply(lambda x: sentiment_analyzer(x,model)))\n",
    "        \n",
    "        # Convert the pandas DataFrame to a list of dictionaries\n",
    "        data_to_insert = batch_df[['_id','sentiment','s_probability']].to_dict('records')\n",
    "\n",
    "        # Insert data into MongoDB\n",
    "        collection.insert_many(data_to_insert)\n",
    "        print(f\"processed {end} of {total_count} - {round((end / total_count)):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6d3f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets in memory\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tweets in memory\")\n",
    "df_pd = df.toPandas()\n",
    "df_pd = df_pd[['_id','text']]\n",
    "df_pd['text'] = df_pd['text'].apply(lambda x: parse_tweet(x))\n",
    "\n",
    "# Add index to your dataframe\n",
    "df_pd['index'] = df_pd.index\n",
    "\n",
    "\n",
    "# Start client connection to mongo\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[\"ca2\"]\n",
    "collection = db[\"vaccin_tweets_2_202004_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5549f9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch process\n",
    "batch_size = 2000\n",
    "start_index = 0\n",
    "end_index = df_pd['index'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31638613-e6f7-4cc5-9fd6-ab10acb4462f",
   "metadata": {},
   "source": [
    "# Generate Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc419c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process_data\n",
      "processing from 0 to 2000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:15<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 2000 of 15162 - 0.00%\n",
      "processing from 2000 to 4000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:35<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 4000 of 15162 - 0.00%\n",
      "processing from 4000 to 6000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:48<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 6000 of 15162 - 0.00%\n",
      "processing from 6000 to 8000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:51<00:00,  8.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 8000 of 15162 - 100.00%\n",
      "processing from 8000 to 10000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:01<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000 of 15162 - 100.00%\n",
      "processing from 10000 to 12000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:42<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 12000 of 15162 - 100.00%\n",
      "processing from 12000 to 14000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:25<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 14000 of 15162 - 100.00%\n",
      "processing from 14000 to 16000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1162/1162 [02:19<00:00,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 16000 of 15162 - 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting process_data\")\n",
    "process_data(df_pd,start_index,end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89dcdf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****End******\n"
     ]
    }
   ],
   "source": [
    "print(f\"*****End******\")\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a57f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
